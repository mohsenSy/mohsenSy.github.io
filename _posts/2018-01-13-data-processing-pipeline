---
layout: post
title:  "Data Processing Pipeline"
date:   2018-01-13 08:58:00 +0300
categories: sysadmin
---

Companies around the world generate large amounts of data every day, this data
need to be stored, processed and analyzed to make sense of it, without analyzing
the data it will not be worth generating it at all, the results of these analytics
will be used to improve the company's products and user's experience.

However generating this big amount of data, storing it, processing and analyzing
it is not a trivial task at all, it needs a low latency, scalable, cost efficient
and highly available infrastructure, in this article I will explain the requirements
of this infrastructure and give an example of my own one which I created based
on my three years experience of working as Linux Systems Administrator.

### Introduction
A Data Processing Pipeline can be used to store, process and analyse log data from
applications, these logs contain many information about user activity in the application,
errors, warnings etc...

They are a valuable resource for investigating bugs and learning about user's activity
which can be used later to improve user's experience.

## infrastructure requirements for a Data Processing Pipeline
* Low Latency: When ingesting logs from the application to the system this must
happen as quickly as possible and not delay response from the server to achieve
this I used rabbitmq server to send logs from the application to it directly, rabbitmq
will be ran on the same server as the application server to minimize delay.
* Scalability: Today the application could be ingesting hundreds of logs per hour
but in the future as the application becomes more and more famous we need the system
to be able to scale with the increased traffic this can be achieved by using multiple
logstash servers and apache kafka cluster to handle large amounts of logs per hour
these tools enable the system to scale easily with increased load.
* High Availabilty: The process of ingesting logs to the system must continue even
in the case of a failure of some servers in the system as the failure of the system
could cause the failure of the entire application which is unacceptable, currently
rabbitmq failures cause the server to be remved from the application cluster, in case
of logstash failure the logs remain in rabbitmq until it comes back online and sent
to it, right now I have a problem with kafka failures which cause data loss however
I will work to solve this problem soon.
* Cost Effective: Adding more servers to existing infrastructure requires more money
however having a Data Processing Pipeline can help greatly to react quickly to errors
have a better idea about user's bahaviour when using the application which can be
used in improving the areas of application where users visit more and working on
the areas where users visit less, also when traffic increases and number of logs
increases the company's income should increase and will be able to handle more
costs to scale the infrastructure to handle more logs.

## Infrastructure Description
In this section I will describe the infrastructure used here to create the data processing
pipeline which consists of five components, [rabbitmq](http://www.rabbitmq.com),
[logstash](https://www.elastic.co/products/logstash), [kafka](https://kafka.apache.org),
[elasticsearch](https://www.elastic.co/products/elasticsearch) and [kibana](https://www.elastic.co/products/kibana).

Each one of these five components plays a role in achieving the requirements mentioned
in the last section.

The following steps show the data flow in the pipeline from application to visulaization

1. First the application send logs to rabbitmq servers installed on the same host as the
   application servers
2. The log data is sent to logstash servers which converts it to JSON and sends them to
   kafka servers for further processing.
3. When data arrive at kafka servers it is processed using kafka streams applications, here
   any needed changes are applied to the data for example: replace numeric IDs with Strings etc...
4. After data is processed it is sent to its final destination elasticsearch servers to be
   stored there and later visulized with kibana dashboards and visulaizations.

Hint: I used [Confluent](https://www.confluent.io) platform to run kafka servers.

In the following sections I will desribe the installation and configuration of each component
using the proper commands.

### Rabbitmq
Rabbitmq is the most popular open source messaging software it can be used deliver messages
from client applications to other applications, it provides low latency delivery and does
not block the client application at all for further processing which makes it ideal for
receiving messages from client applications at very low latency.

#### Installation
To install rabbitmq issue the following two commands:
```
  wget https://github.com/rabbitmq/rabbitmq-server/releases/download/rabbitmq_v3_6_14/rabbitmq-server_3.6.14-1_all.deb
  sudo dpkg -i rabbitmq-server_3.6.14-1_all.deb
```
These two commands install rabbitmq version 3.6.14 on the server you can start the server using this command `sudo service rabbitmq start`

Make sure it is up and running with the following command `sudo service rabbitmq status`

#### Configuration
After rabbitmq is installed you need to configure it first delete the guest default user with the following command

`sudo rabbitmqctl delete_user guest`

Now create a rabbitmq virtual host which will be used for all the configurations used here

`sudo rabbitmqctl add_vhost /log`

Create a user and grant it full permissions on the above virtual host and give it the
administrator tag.
```
  sudo rabbitmqctl add_user logger logger_pass
  sudo rabbitmqctl set_permissions -p /log logger ".*" ".*" ".*"
  sudo rabbitmqctl set_user_tags logger administrator
```

The next step is to enable the rabbitmq management plugin so we can use rabbitmqadmin
to create an exchange

```
  sudo rabbitmq-plugins enable rabbitmq_management
```

Next create a rabbitmq exchange which will be used to route log messages from the application
to the appropriate queue to be picked up later by logstash servers.

```
  sudo rabbitmqadmin declare exchange name=logging type=fanout vhost=/log
```

Now we need to create a rabbitmq queue and bind it with the exchange so any messages
sent to the exchange will be routed to it

```
  sudo rabbitmqadmin declare queue name=logs durable=true
  sudo rabbitmqadmin declare binding source=logging destination_type=queue destination=logs
```

Now rabbitmq is ready to receive log messages from applications and send them to logstash
servers once they are configured to read logs from rabbitmq, I will describe this in the
next section.

### logstash
Logstash is an open source server side data processing pipeline, it can receive data
from various inputs (rabbitmq in our case here) transform them (to JSON here) and then
send them to various outputs (kafka in our case here).

Here logstash is used to filter the required fields from each log message and send them
to kafka topics where they are further processed before sending them finally to elasticsearch.

#### Installation
To install logstash execute these commands

```
  wget https://artifacts.elastic.co/downloads/logstash/logstash-5.6.2.deb
  sudo dpkg -i logstash-5.6.2.deb
```

Now logstash is installed and ready to be configured.

#### Configuration
We will configure logstash using a single file to define inputs, filters and outputs.

Create a new file using your favourite editor in this path `/etc/logstash/conf.d/log.conf`

```
  input {
      rabbitmq {
            host => "<rabbitmq_host_ip>"
            port => 5672
            queue => "logs"
            durable => true
            passive => true
            exchange => "logging"
            user => "logger"
            password => "logger_pass"
            vhost => "/log"
      }
}

filter {
  if [@metadata][rabbitmq_properties][datetime] {
  date {
        match => ["[@metadata][rabbitmq_properties][datetime]", "UNIX"]
  }
  }
  json {
        source => "%{@timestamp} - %{level_name}.%{channel} - %{context} - %{message} - %{extra}"
  }
}

output {
    kafka {
          topic_id => log_topic
          codec => "json"
          bootstrap_servers => "<kafka_server_ip:<kafka_port>"
    }
}
```

From the above file we see three main sections, the first one is the **input** section
here we are using rabbitmq input type to read messages from rabbitmq log queue.

The second one is the **filter** section, here we use date filter to parse the date
and use the json filter to convert log message to a json object using the required properties.

The third one is the **output** section where we define kafka output and send messages
to **logs_input** topic.
